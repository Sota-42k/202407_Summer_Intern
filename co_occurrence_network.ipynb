{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 環境構築（Environment Setup）"
      ],
      "metadata": {
        "id": "QfAQazoGw_8E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_XCfXfbJsjP"
      },
      "outputs": [],
      "source": [
        "# ライブラリ導入\n",
        "!pip install mecab-python3\n",
        "!pip install unidic\n",
        "!python -m unidic download\n",
        "!pip install pyvis==0.2.1\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import unicodedata\n",
        "import MeCab\n",
        "from collections import Counter\n",
        "import requests\n",
        "import matplotlib.pyplot as plt\n",
        "import unidic\n",
        "import re\n",
        "import os\n",
        "import itertools\n",
        "from pyvis.network import Network\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Google Drive マウント\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 作業ディレクトリの変更\n",
        "os.chdir('/content/drive/MyDrive/User')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# データプロセシング（Data Processing）"
      ],
      "metadata": {
        "id": "ja4f7YqWxGwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# CSV取得\n",
        "cs_hist_df = pd.read_csv('cs_hist.csv', encoding = 'cp932')\n",
        "incident_mgmt_df = pd.read_csv('incident_mgmt.csv', encoding = 'cp932')\n",
        "\n",
        "# データクレンジング関数の宣言\n",
        "def replace_words(text, word_to_remove, word_to_replace):\n",
        "  return text.replace(word_to_remove, word_to_replace)\n",
        "\n",
        "# データクレンジング実行\n",
        "word_to_remove = '\\n'\n",
        "word_to_replace = ''\n",
        "cs_hist_df_edit = cs_hist_df.applymap(lambda x: replace_words(x, word_to_remove, word_to_replace) if isinstance(x, str) else x).fillna('').loc[:, ['問合わせ番号', '件名']]\n",
        "incident_mgmt_df_edit = incident_mgmt_df.applymap(lambda x: replace_words(x, word_to_remove, word_to_replace) if isinstance(x, str) else x).fillna('').loc[:, ['問合せ番号']]\n",
        "\n",
        "# 顧客対応からインシデントのみ抽出\n",
        "cs_incident_df = cs_hist_df_edit[cs_hist_df_edit['問合わせ番号'].isin(incident_mgmt_df_edit['問合せ番号'])].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "uYdduC9a_5jn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 形態素解析（Morphological Analysis）"
      ],
      "metadata": {
        "id": "4_-z6Vr3xM_8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 形態素解析関数の宣言\n",
        "def morpheme_tokenizer(text_input):\n",
        "  # 形態素解析関数の宣言\n",
        "  def morpheme(text):\n",
        "    mecab = MeCab.Tagger()\n",
        "    cell_parse = mecab.parse(text)\n",
        "    lines = cell_parse.splitlines()\n",
        "    # EOS（End Of Sentence）の削除\n",
        "    lines = lines[:-1]\n",
        "    data = []\n",
        "    # 各カラムの分離\n",
        "    for line in lines:\n",
        "      surface, feature = line.split('\\t')\n",
        "      feature = [None if f == '*' else f for f in feature.split(',')]\n",
        "      data.append([surface, *feature])\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "  # 形態素解析の実行及びデータフレーム化\n",
        "  words_df = morpheme(text_input)\n",
        "\n",
        "  # 表層形・品詞・原形のみ抽出\n",
        "  if not words_df.empty:\n",
        "    surface_parts_df = words_df.iloc[:, 0:2].reset_index(drop=True)\n",
        "    original = words_df.iloc[:, 8:9].reset_index(drop=True)\n",
        "    words_df_edit = pd.concat([surface_parts_df, original], axis=1)\n",
        "\n",
        "    COLUMNS = ['表層形', '品詞', '原形']\n",
        "    words_df_edit.columns = COLUMNS\n",
        "\n",
        "    # 名詞・形容詞・動詞・副詞のみ抽出\n",
        "    words_df_edit = words_df_edit[words_df_edit['品詞'].isin(['名詞', '形容詞', '動詞', '副詞'])]\n",
        "\n",
        "    # 形態素解析結果をリスト化\n",
        "    word_cloud_list = list(words_df_edit['原形'].dropna())\n",
        "\n",
        "    # 漢字のみ抽出\n",
        "    def remove_eng(word):\n",
        "        return re.sub(r'-[a-zA-Z]+', '', word)\n",
        "    kana_re = re.compile(\"[^あ-ゖ]\")\n",
        "    word_cloud_list_edit = [s for s in word_cloud_list if kana_re.match(s)]\n",
        "    word_cloud_list_edit = [remove_eng(t) for t in word_cloud_list_edit]\n",
        "    return word_cloud_list_edit\n",
        "  else:\n",
        "    return []"
      ],
      "metadata": {
        "id": "TWTQgVoMcb2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 共起ネットワーク"
      ],
      "metadata": {
        "id": "fwDjLnrbxzKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# コロケーションのカウント\n",
        "cells = [morpheme_tokenizer(cell) for cell in cs_incident_df['件名']]\n",
        "cells_combs = [list(itertools.combinations(cell,2)) for cell in cells]\n",
        "words_combs = [[tuple(sorted(words)) for words in cell] for cell in cells_combs]\n",
        "target_combs = []\n",
        "for words_comb in words_combs:\n",
        "  target_combs.extend(words_comb)\n",
        "\n",
        "combs_count = Counter(target_combs)\n",
        "\n",
        "combs_df = pd.DataFrame([{\"前\" : i[0][0], \"後\": i[0][1], \"count\":i[1]} for i in combs_count.most_common()])"
      ],
      "metadata": {
        "id": "NiqtIhSqYBIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 共起ネットワーク関数の宣言\n",
        "def co_occ_network(df):\n",
        "  got_net = Network(\n",
        "    height = '500px',\n",
        "    width = '95%',\n",
        "    bgcolor=\"white\",\n",
        "    font_color=\"black\",\n",
        "    notebook=True\n",
        "    )\n",
        "\n",
        "  got_net.force_atlas_2based()\n",
        "  got_data = df[:150]\n",
        "\n",
        "  sources = got_data['前']\n",
        "  targets = got_data['後']\n",
        "  weights = got_data['count']\n",
        "\n",
        "  edge_data = zip(sources, targets, weights)\n",
        "\n",
        "  for e in edge_data:\n",
        "    src = e[0]\n",
        "    dst = e[1]\n",
        "    w = e[2]\n",
        "\n",
        "    got_net.add_node(src, src, title=src)\n",
        "    got_net.add_node(dst, dst, title=dst)\n",
        "    got_net.add_edge(src, dst, value=w)\n",
        "\n",
        "  neighbor_map = got_net.get_adj_list()\n",
        "\n",
        "  for node in got_net.nodes:\n",
        "    node[\"title\"] += \" Neighbors:<br>\" + \"<br>\".join(neighbor_map[node[\"id\"]])\n",
        "    node[\"value\"] = len(neighbor_map[node[\"id\"]])\n",
        "\n",
        "  got_net.show_buttons(filter_=['physics'])\n",
        "  return got_net"
      ],
      "metadata": {
        "id": "ygwEN8Ydx7_8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 共起ネットワークの表示\n",
        "got_net = co_occ_network(combs_df)\n",
        "got_net.show('co_occurrence_network.html')\n",
        "\n",
        "display(HTML('co_occurrence_network.html'))"
      ],
      "metadata": {
        "id": "atSjb1Apx_jG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}